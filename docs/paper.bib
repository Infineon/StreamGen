
@article{masana_class-incremental_2022,
	title = {Class-{{Incremental Learning}}: {{Survey}} and {{Performance Evaluation}} on {{Image Classification}}},
	shorttitle = {Class-{{Incremental Learning}}},
	author = {Masana, Marc and Liu, Xialei and Twardowski, Bartłomiej and Menta, Mikel and Bagdanov, Andrew D. and family=Weijer, given=Joost, prefix=van de, useprefix=true},
	date = {2023-05},
	journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume = {45},
	number = {5},
	pages = {5513--5533},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2022.3213473},
	url = {https://ieeexplore.ieee.org/document/9915459},
	urldate = {2024-10-08},
	abstract = {For future learning systems, incremental learning is desirable because it allows for: efficient resource usage by eliminating the need to retrain from scratch at the arrival of new data; reduced memory usage by preventing or limiting the amount of data required to be stored – also important when privacy limitations are imposed; and learning that more closely resembles human learning. The main challenge for incremental learning is catastrophic forgetting, which refers to the precipitous drop in performance on previously learned tasks after learning a new one. Incremental learning of deep neural networks has seen explosive growth in recent years. Initial work focused on task-incremental learning, where a task-ID is provided at inference time. Recently, we have seen a shift towards class-incremental learning where the learner must discriminate at inference time between all classes seen in previous tasks without recourse to a task-ID. In this paper, we provide a complete survey of existing class-incremental learning methods for image classification, and in particular, we perform an extensive experimental evaluation on thirteen class-incremental methods. We consider several new experimental scenarios, including a comparison of class-incremental methods on multiple large-scale image classification datasets, an investigation into small and large domain shifts, and a comparison of various network architectures.},
	eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
}

@article{lu_learning_2018,
	title = {Learning under {Concept} {Drift}: {A} {Review}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {Learning under {Concept} {Drift}},
	url = {http://arxiv.org/abs/2004.05785},
	doi = {10.1109/TKDE.2018.2876857},
	abstract = {Concept drift describes unforeseeable changes in the underlying distribution of streaming data over time. Concept drift research involves the development of methodologies and techniques for drift detection, understanding and adaptation. Data analysis has revealed that machine learning in a concept drift environment will result in poor learning results if the drift is not addressed. To help researchers identify which research topics are significant and how to apply related techniques in data analysis tasks, it is necessary that a high quality, instructive review of current research developments and trends in the concept drift field is conducted. In addition, due to the rapid development of concept drift in recent years, the methodologies of learning under concept drift have become noticeably systematic, unveiling a framework which has not been mentioned in literature. This paper reviews over 130 high quality publications in concept drift related research areas, analyzes up-to-date developments in methodologies and techniques, and establishes a framework of learning under concept drift including three main components: concept drift detection, concept drift understanding, and concept drift adaptation. This paper lists and discusses 10 popular synthetic datasets and 14 publicly available benchmark datasets used for evaluating the performance of learning algorithms aiming at handling concept drift. Also, concept drift related research directions are covered and discussed. By providing state-of-the-art knowledge, this survey will directly support researchers in their understanding of research developments in the field of learning under concept drift.},
	urldate = {2023-08-07},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Lu, Jie and Liu, Anjin and Dong, Fan and Gu, Feng and Gama, Joao and Zhang, Guangquan},
	year = {2018},
	note = {arXiv:2004.05785 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--1},
}

@inproceedings{hess_procedural_2021,
	title = {A {{Procedural World Generation Framework}} for {{Systematic Evaluation}} of {{Continual Learning}}},
	author = {Hess, Timm and Mundt, Martin and Pliushch, Iuliia and Ramesh, Visvanathan},
	date = {2021-06-08},
	url = {https://openreview.net/forum?id=LlCQWh8-pwK},
	urldate = {2024-10-08},
	abstract = {Several families of continual learning techniques have been proposed to alleviate catastrophic interference in deep neural network training on non-stationary data. However, a comprehensive comparison and analysis of limitations remains largely open due to the inaccessibility to suitable datasets. Empirical examination not only varies immensely between individual works, it further currently relies on contrived composition of benchmarks through subdivision and concatenation of various prevalent static vision datasets. In this work, our goal is to bridge this gap by introducing a computer graphics simulation framework that repeatedly renders only upcoming urban scene fragments in an endless real-time procedural world generation process. At its core lies a modular parametric generative model with adaptable generative factors. The latter can be used to flexibly compose data streams, which significantly facilitates a detailed analysis and allows for effortless investigation of various continual learning schemes.},
	eventtitle = {Thirty-Fifth {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}} ({{Round}} 1)},
	langid = {english},
}

@article{cossu_is_2021,
	title = {Is {{Class-Incremental Enough}} for {{Continual Learning}}?},
	author = {Cossu, Andrea and Graffieti, Gabriele and Pellegrini, Lorenzo and Maltoni, Davide and Bacciu, Davide and Carta, Antonio and Lomonaco, Vincenzo},
	date = {2022-03-24},
	journaltitle = {Frontiers in Artificial Intelligence},
	shortjournal = {Front. Artif. Intell.},
	volume = {5},
	publisher = {Frontiers},
	issn = {2624-8212},
	doi = {10.3389/frai.2022.829842},
	url = {https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2022.829842/full},
	urldate = {2024-10-08},
	abstract = {{$<$}p{$>$}The ability of a model to learn continually can be empirically assessed in different continual learning scenarios. Each scenario defines the constraints and the opportunities of the learning environment. Here, we challenge the current trend in the continual learning literature to experiment mainly on class-incremental scenarios, where classes present in one experience are never revisited. We posit that an excessive focus on this setting may be limiting for future research on continual learning, since class-incremental scenarios artificially exacerbate catastrophic forgetting, at the expense of other important objectives like forward transfer and computational efficiency. In many real-world environments, in fact, repetition of previously encountered concepts occurs naturally and contributes to softening the disruption of previous knowledge. We advocate for a more in-depth study of alternative continual learning scenarios, in which repetition is integrated by design in the stream of incoming information. Starting from already existing proposals, we describe the advantages such {$<$}italic{$>$}class-incremental with repetition{$<$}/italic{$>$} scenarios could offer for a more comprehensive assessment of continual learning models.{$<$}/p{$>$}},
	langid = {english},
	keywords = {catastrophic-forgetting,class-incremental,class-incremental-with-repetition,continual-learning,Lifelong-learning},
}

@article{wu_wafer_2015,
	title = {Wafer {Map} {Failure} {Pattern} {Recognition} and {Similarity} {Ranking} for {Large}-{Scale} {Data} {Sets}},
	volume = {28},
	issn = {1558-2345},
	url = {https://ieeexplore.ieee.org/document/6932449},
	doi = {10.1109/TSM.2014.2364237},
	abstract = {Wafer maps can exhibit specific failure patterns that provide crucial details for assisting engineers in identifying the cause of wafer pattern failures. Conventional approaches of wafer map failure pattern recognition (WMFPR) and wafer map similarity ranking (WMSR) generally involve applying raw wafer map data (i.e., without performing feature extraction). However, because increasingly more sensor data are analyzed during semiconductor fabrication, currently used approaches can be inadequate in processing large-scale data sets. Therefore, a set of novel rotation- and scale-invariant features is proposed for obtaining a reduced representation of wafer maps. Such features are crucial when employing WMFPR and WMSR to analyze large-scale data sets. To validate the performance of the proposed system, the world's largest publicly accessible data set of wafer maps was built, comprising 811 457 real-world wafer maps. The experimental results show that the proposed features and overall system can process large-scale data sets effectively and efficiently, thereby meeting the requirements of current semiconductor fabrication.},
	number = {1},
	urldate = {2024-01-30},
	journal = {IEEE Transactions on Semiconductor Manufacturing},
	author = {Wu, Ming-Ju and Jang, Jyh-Shing R. and Chen, Jui-Long},
	month = feb,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Semiconductor Manufacturing},
	keywords = {Feature extraction, Semiconductor device modeling, Data models, Support vector machines, Pattern recognition, semiconductor defects, Fabrication, pattern recognition, data models, image recognition, information retrieval, Transforms},
	pages = {1--12},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Hundgeburth\\Zotero\\storage\\V7KLV8YF\\6932449.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Hundgeburth\\Zotero\\storage\\RZAXXD2B\\Wu et al. - 2015 - Wafer Map Failure Pattern Recognition and Similari.pdf:application/pdf},
}

@misc{van_de_ven_continual_2024,
	title = {Continual {Learning} and {Catastrophic} {Forgetting}},
	url = {http://arxiv.org/abs/2403.05175},
	doi = {10.48550/arXiv.2403.05175},
	abstract = {This book chapter delves into the dynamics of continual learning, which is the process of incrementally learning from a non-stationary stream of data. Although continual learning is a natural skill for the human brain, it is very challenging for artificial neural networks. An important reason is that, when learning something new, these networks tend to quickly and drastically forget what they had learned before, a phenomenon known as catastrophic forgetting. Especially in the last decade, continual learning has become an extensively studied topic in deep learning. This book chapter reviews the insights that this field has generated.},
	language = {en},
	urldate = {2024-03-15},
	publisher = {arXiv},
	author = {van de Ven, Gido M. and Soures, Nicholas and Kudithipudi, Dhireesha},
	month = mar,
	year = {2024},
	note = {arXiv:2403.05175 [cs, q-bio, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	file = {2403.05175v1.pdf:C\:\\Users\\Hundgeburth\\Zotero\\storage\\TZ2HLTKM\\2403.05175v1.pdf:application/pdf},
}

@inproceedings{lomonaco_avalanche_2021,
	address = {Nashville, TN, USA},
	title = {Avalanche: an {End}-to-{End} {Library} for {Continual} {Learning}},
	isbn = {978-1-66544-899-4},
	shorttitle = {Avalanche},
	url = {https://ieeexplore.ieee.org/document/9523188/},
	doi = {10.1109/CVPRW53098.2021.00399},
	abstract = {Learning continually from non-stationary data streams is a long-standing goal and a challenging problem in machine learning. Recently, we have witnessed a renewed and fast-growing interest in continual learning, especially within the deep learning community. However, algorithmic solutions are often difﬁcult to re-implement, evaluate and port across different settings, where even results on standard benchmarks are hard to reproduce. In this work, we propose Avalanche, an open-source end-to-end library for continual learning research based on PyTorch. Avalanche is designed to provide a shared and collaborative codebase for fast prototyping, training, and reproducible evaluation of continual learning algorithms.},
	language = {en},
	urldate = {2024-03-18},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Lomonaco, Vincenzo and Pellegrini, Lorenzo and Cossu, Andrea and Carta, Antonio and Graffieti, Gabriele and Hayes, Tyler L. and De Lange, Matthias and Masana, Marc and Pomponi, Jary and Van De Ven, Gido M. and Mundt, Martin and She, Qi and Cooper, Keiland and Forest, Jeremy and Belouadah, Eden and Calderara, Simone and Parisi, German I. and Cuzzolin, Fabio and Tolias, Andreas S. and Scardapane, Simone and Antiga, Luca and Ahmad, Subutai and Popescu, Adrian and Kanan, Christopher and Van De Weijer, Joost and Tuytelaars, Tinne and Bacciu, Davide and Maltoni, Davide},
	month = jun,
	year = {2021},
	pages = {3595--3605},
	file = {Lomonaco et al. - 2021 - Avalanche an End-to-End Library for Continual Lea.pdf:C\:\\Users\\Hundgeburth\\Zotero\\storage\\7XKNWR72\\Lomonaco et al. - 2021 - Avalanche an End-to-End Library for Continual Lea.pdf:application/pdf},
}

@misc{douillard_continuum_2021,
	title = {Continuum: {Simple} {Management} of {Complex} {Continual} {Learning} {Scenarios}},
	shorttitle = {Continuum},
	url = {http://arxiv.org/abs/2102.06253},
	doi = {10.48550/arXiv.2102.06253},
	abstract = {Continual learning is a machine learning sub-field specialized in settings with non-iid data. Hence, the training data distribution is not static and drifts through time. Those drifts might cause interferences in the trained model and knowledge learned on previous states of the data distribution might be forgotten. Continual learning's challenge is to create algorithms able to learn an ever-growing amount of knowledge while dealing with data distribution drifts. One implementation difficulty in these field is to create data loaders that simulate non-iid scenarios. Indeed, data loaders are a key component for continual algorithms. They should be carefully designed and reproducible. Small errors in data loaders have a critical impact on algorithm results, e.g. with bad preprocessing, wrong order of data or bad test set. Continuum is a simple and efficient framework with numerous data loaders that avoid researcher to spend time on designing data loader and eliminate time-consuming errors. Using our proposed framework, it is possible to directly focus on the model design by using the multiple scenarios and evaluation metrics implemented. Furthermore the framework is easily extendable to add novel settings for specific needs.},
	urldate = {2024-03-18},
	publisher = {arXiv},
	author = {Douillard, Arthur and Lesort, Timothée},
	month = feb,
	year = {2021},
	note = {arXiv:2102.06253 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Hundgeburth\\Zotero\\storage\\QWJPAXMI\\Douillard and Lesort - 2021 - Continuum Simple Management of Complex Continual .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Hundgeburth\\Zotero\\storage\\QALTU8MW\\2102.html:text/html},
}

@misc{hendrycks_benchmarking_2019,
	title = {Benchmarking {{Neural Network Robustness}} to {{Common Corruptions}} and {{Perturbations}}},
	author = {Hendrycks, Dan and Dietterich, Thomas},
	date = {2018-09-27},
	url = {https://openreview.net/forum?id=HJz6tiCqYm},
	urldate = {2024-10-08},
	abstract = {In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.},
	eventtitle = {International {{Conference}} on {{Learning Representations}}},
	langid = {english},
}

@article{paszke_pytorch_nodate,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientiﬁc computing libraries, while remaining efﬁcient and supporting hardware accelerators such as GPUs.},
	language = {en},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	file = {Paszke et al. - PyTorch An Imperative Style, High-Performance Dee.pdf:C\:\\Users\\Hundgeburth\\Zotero\\storage\\XZP46DKD\\Paszke et al. - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf},
}

@software{torchvision2016,
    title = {{{TorchVision}}: {{PyTorch}}'s {{Computer Vision}} Library},
    author = {TorchVision maintainers and contributors},
    date = {2016-11},
    url = {https://github.com/pytorch/vision}
}

@misc{c0fec0de_anytree_2016,
	title = {anytree: {Python} tree data library},
	url = {https://github.com/c0fec0de/anytree},
	publisher = {GitHub},
	author = {{c0fec0de}},
	year = {2016},
	note = {Publication Title: GitHub repository},
}

@article{gansner_open_1997,
	title = {An {Open} {Graph} {Visualization} {System} and {Its} {Applications} to {Software} {Engineering}},
	volume = {30},
	doi = {10.1002/1097-024X(200009)30:11<1203::AID-SPE338>3.0.CO;2-N},
	journal = {Software - Practice and Experience - SPE},
	author = {Gansner, Emden and North, Stephen},
	month = jan,
	year = {1997},
}
